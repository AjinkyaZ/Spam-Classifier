{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import listdir, path\n",
    "import json\n",
    "import re\n",
    "from math import log\n",
    "from collections import Counter\n",
    "from __future__ import division\n",
    "\n",
    "stoplist = set([u'all', u'six', u'just', u'less', u'being', u'indeed', u'over', u'move', u'anyway', u'four', u'not', u'own', u'through', u'using', u'fify', u'where', u'mill', u'only', u'find', u'before', u'one', u'whose', u'system', u'how', u'somewhere', u'much', u'thick', u'show', u'had', u'enough', u'should', u'to', u'must', u'whom', u'seeming', u'yourselves', u'under', u'ours', u'two', u'has', u'might', u'thereafter', u'latterly', u'do', u'them', u'his', u'around', u'than', u'get', u'very', u'de', u'none', u'cannot', u'every', u'un', u'they', u'front', u'during', u'thus', u'now', u'him', u'nor', u'name', u'regarding', u'several', u'hereafter', u'did', u'always', u'who', u'didn', u'whither', u'this', u'someone', u'either', u'each', u'become', u'thereupon', u'sometime', u'side', u'towards', u'therein', u'twelve', u'because', u'often', u'ten', u'our', u'doing', u'km', u'eg', u'some', u'back', u'used', u'up', u'go', u'namely', u'computer', u'are', u'further', u'beyond', u'ourselves', u'yet', u'out', u'even', u'will', u'what', u'still', u'for', u'bottom', u'mine', u'since', u'please', u'forty', u'per', u'its', u'everything', u'behind', u'does', u'various', u'above', u'between', u'it', u'neither', u'seemed', u'ever', u'across', u'she', u'somehow', u'be', u'we', u'full', u'never', u'sixty', u'however', u'here', u'otherwise', u'were', u'whereupon', u'nowhere', u'although', u'found', u'alone', u're', u'along', u'quite', u'fifteen', u'by', u'both', u'about', u'last', u'would', u'anything', u'via', u'many', u'could', u'thence', u'put', u'against', u'keep', u'etc', u'amount', u'became', u'ltd', u'hence', u'onto', u'or', u'con', u'among', u'already', u'co', u'afterwards', u'formerly', u'within', u'seems', u'into', u'others', u'while', u'whatever', u'except', u'down', u'hers', u'everyone', u'done', u'least', u'another', u'whoever', u'moreover', u'couldnt', u'throughout', u'anyhow', u'yourself', u'three', u'from', u'her', u'few', u'together', u'top', u'there', u'due', u'been', u'next', u'anyone', u'eleven', u'cry', u'call', u'therefore', u'interest', u'then', u'thru', u'themselves', u'hundred', u'really', u'sincere', u'empty', u'more', u'himself', u'elsewhere', u'mostly', u'on', u'fire', u'am', u'becoming', u'hereby', u'amongst', u'else', u'part', u'everywhere', u'too', u'kg', u'herself', u'former', u'those', u'he', u'me', u'myself', u'made', u'twenty', u'these', u'was', u'bill', u'cant', u'us', u'until', u'besides', u'nevertheless', u'below', u'anywhere', u'nine', u'can', u'whether', u'of', u'your', u'toward', u'my', u'say', u'something', u'and', u'whereafter', u'whenever', u'give', u'almost', u'wherever', u'is', u'describe', u'beforehand', u'herein', u'doesn', u'an', u'as', u'itself', u'at', u'have', u'in', u'seem', u'whence', u'ie', u'any', u'fill', u'again', u'hasnt', u'inc', u'thereby', u'thin', u'no', u'perhaps', u'latter', u'meanwhile', u'when', u'detail', u'same', u'wherein', u'beside', u'also', u'that', u'other', u'take', u'which', u'becomes', u'you', u'if', u'nobody', u'unless', u'whereas', u'see', u'though', u'may', u'after', u'upon', u'most', u'hereupon', u'eight', u'but', u'serious', u'nothing', u'such', u'why', u'off', u'a', u'don', u'whereby', u'third', u'i', u'whole', u'noone', u'sometimes', u'well', u'amoungst', u'yours', u'their', u'rather', u'without', u'so', u'five', u'the', u'first', u'with', u'make', u'once'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train set has files from enron1/ham, enron1/spam, spamassassin/easy_ham, spamassassin/spam\n",
    "train_spam = './train/spam'\n",
    "train_ham = './train/ham'\n",
    "spam_folder = [path.join(train_spam, fi) for fi in listdir(train_spam) if path.isfile(path.join(train_spam, fi))]\n",
    "ham_folder = [path.join(train_ham, fi) for fi in listdir(train_ham) if path.isfile(path.join(train_ham, fi))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    p_words = re.compile(r'[\\w]+')\n",
    "    f = open(file_name, 'r')\n",
    "    file_str = ''\n",
    "    for i in f: file_str += i\n",
    "    if file_str.strip() == '': return\n",
    "    file_words = p_words.findall(file_str)\n",
    "    file_words = [word.lower() for word in file_words if word.lower() not in stoplist]\n",
    "    return file_words\n",
    "\n",
    "spam_files = [(read_file(fi), 'SPAM') for fi in spam_folder]\n",
    "N_spam = len(spam_files)\n",
    "ham_files = [(read_file(fi), 'HAM') for fi in ham_folder]\n",
    "N_ham = len(ham_files)\n",
    "all_files = spam_files+ham_files\n",
    "N_files = len(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam files : 2001\n",
      "Ham files : 6223\n",
      "Total files : 8224\n"
     ]
    }
   ],
   "source": [
    "print \"Spam files :\", N_spam\n",
    "print \"Ham files :\", N_ham\n",
    "print \"Total files :\", N_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of terms in all files : 2127113\n",
      "Size of vocab : 120366\n",
      "Terms in spam files : 565152\n",
      "Spam vocab : 76060\n",
      "Terms in ham files : 1561961\n",
      "Ham vocab : 59664\n"
     ]
    }
   ],
   "source": [
    "terms_in_all = [term for fx in all_files for term in fx[0]]\n",
    "vocab = Counter(terms_in_all)\n",
    "print \"Total number of terms in all files :\", len(terms_in_all)\n",
    "print \"Size of vocab :\", len(vocab)\n",
    "\n",
    "terms_in_spam = [term for fs in spam_files for term in fs[0]]\n",
    "counts_spam = Counter(terms_in_spam)\n",
    "print \"Terms in spam files :\", len(terms_in_spam)\n",
    "print \"Spam vocab :\", len(counts_spam)\n",
    "\n",
    "terms_in_ham = [term for fh in ham_files for term in fh[0]]\n",
    "counts_ham = Counter(terms_in_ham)\n",
    "print \"Terms in ham files :\", len(terms_in_ham)\n",
    "print \"Ham vocab :\", len(counts_ham)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam prior probability : 0.243312256809\n",
      "Ham prior probability : 0.756687743191\n"
     ]
    }
   ],
   "source": [
    "P_spam = N_spam/float(N_files)\n",
    "print \"Spam prior probability :\", P_spam\n",
    "P_ham = N_ham/float(N_files)\n",
    "print \"Ham prior probability :\", P_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cond_prob = {'spam': {}, 'ham': {}}\n",
    "score_spam = log(P_spam)\n",
    "score_ham = log(P_ham)\n",
    "for term in vocab:\n",
    "    term_spam_count = counts_spam[term] \n",
    "    term_ham_count = counts_ham[term]\n",
    "    cond_prob['spam'][term] = (term_spam_count+1)/float(len(terms_in_spam)+len(vocab))\n",
    "    cond_prob['ham'][term] = (term_ham_count+1)/float(len(terms_in_ham)+len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SpamClassifier:\n",
    "    def __init__(self):\n",
    "        self.prior_spam = None\n",
    "        self.prior_ham = None\n",
    "        self.likelihood = None\n",
    "        \n",
    "    def classify(self, message_terms):\n",
    "        score_spam = self.prior_spam\n",
    "        score_ham = self.prior_ham\n",
    "        for term in message_terms:\n",
    "            try:\n",
    "                score_spam += log(self.likelihood['spam'][term])\n",
    "            except KeyError, e:\n",
    "                score_spam += log(1/float(len(terms_in_spam)+len(vocab)))\n",
    "            try:\n",
    "                score_ham += log(self.likelihood['ham'][term])\n",
    "            except KeyError, e:\n",
    "                score_ham += log(1/float(len(terms_in_ham)+len(vocab)))\n",
    "        if score_spam > score_ham:\n",
    "            return 1 #SPAM\n",
    "        else:\n",
    "            return 0 #HAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in test set (spam) : 1500\n",
      "Files in test set (ham) : 4012\n",
      "Total files in test set : 5512\n"
     ]
    }
   ],
   "source": [
    "test_spam_path = './enron3/spam'\n",
    "test_ham_path = './enron3/ham'\n",
    "test_spam_folder = [path.join(test_spam_path, fi) for fi in listdir(test_spam_path) if path.isfile(path.join(test_spam_path, fi))]\n",
    "test_spam_files = [(read_file(fi), 1) for fi in test_spam_folder]\n",
    "print \"Files in test set (spam) :\", len(test_spam_files)\n",
    "test_ham_folder = [path.join(test_ham_path, fi) for fi in listdir(test_ham_path) if path.isfile(path.join(test_ham_path, fi))]\n",
    "test_ham_files = [(read_file(fi), 0) for fi in test_ham_folder]\n",
    "print \"Files in test set (ham) :\", len(test_ham_files)\n",
    "test_all = []\n",
    "for i in test_spam_files:\n",
    "    test_all.append((i[0], 1))\n",
    "for i in test_ham_files:\n",
    "    test_all.append((i[0], 0))\n",
    "total_test_files = len(test_spam_files)+len(test_ham_files)\n",
    "print \"Total files in test set :\", total_test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Positives : 1500  Number of Negatives  : 4012\n",
      "True Positives (TP) : 1471  False Positives (FP) : 235\n",
      "True Negatives (TN) : 3777  False Negatives (FN) : 29\n",
      "True Positive Rate (TPR) : 0.981  False Positive Rate (FPR) : 0.059\n"
     ]
    }
   ],
   "source": [
    "results_for_spam = []\n",
    "results_for_ham = []\n",
    "clf = SpamClassifier()\n",
    "clf.prior_spam = log(P_spam)\n",
    "clf.prior_ham = log(P_ham)\n",
    "clf.likelihood = cond_prob\n",
    "n_spam = len(test_spam_files)\n",
    "n_ham = len(test_ham_files)\n",
    "TP=0\n",
    "FP=0\n",
    "TN=0\n",
    "FN=0\n",
    "for i in test_all:\n",
    "    label = i[1]\n",
    "    result = clf.classify(i[0])\n",
    "    if label == 1 and result == 1:   #file spam and classified spam\n",
    "        TP += 1\n",
    "    elif label == 0 and result == 1: #file ham but classifed spam\n",
    "        FP += 1\n",
    "    elif label == 1 and result == 0: #file spam but classifed ham\n",
    "        FN += 1                 \n",
    "    else:                            #file ham and classified ham\n",
    "        TN += 1\n",
    "\n",
    "Total = TP+TN+FP+FN\n",
    "TPR = TP/float(n_spam) #sensitivity\n",
    "FPR = FP/float(n_ham) # 1-specificity\n",
    "print \"Number of Positives :\", n_spam, \" Number of Negatives  :\", n_ham\n",
    "print \"True Positives (TP) :\", TP, \" False Positives (FP) :\", FP\n",
    "print \"True Negatives (TN) :\", TN, \" False Negatives (FN) :\", FN\n",
    "print \"True Positive Rate (TPR) :\", round(TPR, 3), \" False Positive Rate (FPR) :\", round(FPR, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Metrics : \n",
      "Accuracy  : 0.952    Error Rate : 0.048\n",
      "Precision : 0.862    Recall     : 0.981\n",
      "F1-score  : 0.918\n"
     ]
    }
   ],
   "source": [
    "accuracy = (TP+TN)/float(Total)\n",
    "precision =  TP/float(TP+FP)\n",
    "recall = TP/float(TP+FN)\n",
    "f1_score = (2*(precision*recall))/float(precision+recall)\n",
    "error_rate = (FP+FN)/float(Total)\n",
    "print \"Classifier Metrics : \"\n",
    "print \"Accuracy  :\", round(accuracy, 3), \"   Error Rate :\", round(error_rate, 3)\n",
    "print \"Precision :\", round(precision, 3), \"   Recall     :\", round(recall, 3)\n",
    "print \"F1-score  :\", round(f1_score, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
